{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain import hub\n",
    "import textwrap\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from IPython.display import display\n",
    "from langchain.load import dumps, loads\n",
    "from IPython.display import Markdown\n",
    "import glob\n",
    "from FlagEmbedding import FlagReranker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document loader and splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = glob.glob(\"src/*.pdf\")\n",
    "pages = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pages.extend(PyPDFLoader(pdf_file).load_and_split())\n",
    "\n",
    "for page in pages:\n",
    "    page.page_content = page.page_content.replace(\"Imprimerie Officielle de la République Tunisienne\", \"\")\n",
    "    page.page_content = page.page_content.replace(\"/tatweel\", \"\")\n",
    "\n",
    "doc_chunks = []\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=850,\n",
    "    separators=[\"\\nArticle\", \"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "len(chunks)\n",
    "\n",
    "# embedder = HuggingFaceEmbeddings(\n",
    "#     model_name = \"BAAI/bge-m3\"\n",
    "# )\n",
    "\n",
    "\n",
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=chunks,\n",
    "#     embedding=embedder,\n",
    "#     persist_directory=persist_directory\n",
    "# ) \n",
    "# vectordb.persist()\n",
    "\n",
    "# OR\n",
    "\n",
    "# index_name = \"pfa\"\n",
    "# os.environ[\"PINECONE_API_KEY\"] = \"9721efa4-ff98-4bc5-9b28-93919d1657a5\"\n",
    "#docsearch = PineconeVectorStore.from_documents(chunks, embedder, index_name=index_name)\n",
    "#docsearch = PineconeVectorStore(index_name=index_name,embedding=embedder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15880\n"
     ]
    }
   ],
   "source": [
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name = \"BAAI/bge-m3\"\n",
    ")\n",
    "persist_directory = 'db/'\n",
    "vectordb = Chroma(persist_directory=persist_directory,embedding_function=embedder)\n",
    "retriever = vectordb.as_retriever(k=5)\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Multi-Query</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Tu es un assistant juridique. \n",
    "Votre mission consiste à générer cinq versions différentes de la question initiale de l'utilisateur,\n",
    "afin de récupérer des documents pertinents dans une base de données vectorielle. \n",
    "En proposant plusieurs points de vue sur la question de l'utilisateur, votre objectif\n",
    "est de l'aider à surmonter certaines limitations de la recherche de similarité basée sur la distance. \n",
    "Fournissez ces questions alternatives séparées par des sauts de ligne. Question originale: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# model = Ollama(model=\"llama3\")\n",
    "model = Ollama(model=\"mistral\")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | model \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "question = \"Qui peut etre president de la Tunisie?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>RAG-fusion</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Tu es un assistant juridique. \n",
    "Votre mission consiste à générer cinq versions différentes de la question initiale de l'utilisateur,\n",
    "afin de récupérer des documents pertinents dans une base de données vectorielle. \n",
    "En proposant plusieurs points de vue sur la question de l'utilisateur, votre objectif\n",
    "est de l'aider à surmonter certaines limitations de la recherche de similarité basée sur la distance. \n",
    "Fournissez ces questions alternatives séparées par des sauts de ligne. Question originale: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = Ollama(model=\"mistral\")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | model\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results\n",
    "\n",
    "question = \"Qui peut etre president de la Tunisie?\"\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Query-Decomposition</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "template = \"\"\"Tu es un assistant utile qui génère plusieurs sous-questions à partir d'une question initiale.\n",
    "L'objectif est de décomposer la question initiale en un ensemble de sous-problèmes ou de sous-questions pouvant\n",
    "être répondus de manière isolée.\n",
    "Génère des requêtes de recherche multiples liées à : {question}\n",
    "Sortie (3 requêtes) :\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = Ollama(model=\"mistral\")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | model\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking (using bge reranker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking (using bge reranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\n",
    "scores = reranker.compute_score([[query,context.page_content] for context in unique_docs ], normalize=True)\n",
    "print(scores) \n",
    "#rank the context.page_content by the scores\n",
    "reranked_found_docs=sorted(zip(scores,unique_docs),reverse=True)\n",
    "#remove the scores and keep only the context.page_content\n",
    "sorted_found_docs=[doc[1] for doc in reranked_found_docs] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = hub.pull(\"mehdixlabetix/rag-law-fr\")\n",
    "\n",
    "\n",
    "final_prompt = prompt.invoke({\"context\":format_docs(sorted_found_docs), \"question\":query})\n",
    "\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mistral LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list of gemini versions (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini (stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "\n",
    "# response = model.generate_content(final_prompt.messages[0].content)\n",
    "\n",
    "response = model.generate_content(final_prompt.messages[0].content,stream=True)\n",
    "for chunk in response:\n",
    "  print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query safety evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response._result.candidates[0].safety_ratings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
